---
title: Referencia de API de registro de Azure ML | Microsoft Docs
description: Referencia de API de registro.
services: machine-learning
author: akshaya-a
ms.author: akannava
manager: mwinkle
ms.reviewer: garyericson, jasonwhowell, mldocs
ms.service: machine-learning
ms.workload: data-services
ms.topic: article
ms.date: 09/25/2017
ms.openlocfilehash: 1906425c6657fb6232a9dc306b05f9171c9c7bef
ms.sourcegitcommit: 6699c77dcbd5f8a1a2f21fba3d0a0005ac9ed6b7
ms.translationtype: HT
ms.contentlocale: es-ES
ms.lasthandoff: 10/11/2017
---
# <a name="logging-api-reference"></a>Referencia de API de registro

La biblioteca del registro de Azure ML permite que el programa emita las métricas y los archivos seguidos por el servicio de historial para su análisis posterior. Actualmente, se admiten algunos tipos básicos de archivos y métricas y el conjunto de tipos compatibles aumentará con futuras versiones del paquete de Python.

## <a name="uploading-metrics"></a>Métricas de carga

```python
# import logging API package
from azureml.logging import get_azureml_logger

# initialize a logger object
logger = get_azureml_logger()

# log "scalar" metrics
logger.log("simple integer value", 7)
logger.log("simple float value", 3.141592)
logger.log("simple string value", "this is a string metric")

# log a list of numerical values. 
# this automatically creates a chart in the Run History details page
logger.log("chart data points", [1, 3, 5, 10, 6, 4])
```

De manera predeterminada, todas las métricas se envían de forma asíncrona para que el envío no impida la ejecución del programa. Esto puede causar problemas de ordenación cuando se envían varias métricas en casos extremos. Un ejemplo de esto sería dos métricas que se registran al mismo tiempo, pero que, por alguna razón, el usuario prefiere que se conserve su orden exacto. Otro caso es cuando se debe realizar el seguimiento de la métrica antes de ejecutar algún código que se sabe que puede producir potencialmente errores inmediatos. En ambos casos, la solución consiste en _esperar_ hasta que la métrica se registre por completo antes de continuar:

```python
# blocking call
logger.log("my metric 1", 1).wait()
logger.log("my metric 2", 2).wait()
```

## <a name="consuming-metrics"></a>Métricas de consumo

El servicio de historial almacena las métricas y las vincula a la ejecución que las generó. La pestaña Historial de ejecución y el siguiente comando de CLI le permiten recuperarlas (y los siguientes artefactos) una vez completada una ejecución.

```azurecli
# show the last run
$ az ml history last

# list all past runs
$ az ml history list 

# show a paritcular run
$ az ml history info -r <runid>
```

## <a name="artifacts-files"></a>Artefactos (archivos)

Además de las métricas, Azure ML también permite que el usuario realice un seguimiento de los archivos. De forma predeterminada, todos los archivos escritos en la carpeta `outputs` relativa al directorio de trabajo del programa (la carpeta del proyecto en el contexto de proceso) se cargan en el servicio de historial y se realiza su seguimiento para su posterior análisis. La advertencia es que el tamaño de cada archivo debe ser inferior a 512 MB.


```Python
# Log content as an artifact
logger.upload("artifact/path", "This should be the contents of artifact/path in the service")
```

## <a name="consuming-artifacts"></a>Artefactos de consumo

Para imprimir el contenido de un artefacto del que se ha realizado un seguimiento, el usuario puede utilizar la pestaña Historial de ejecución para la ejecución específica para el artefacto **Descargar** o **Promover**, o bien usar los siguientes comandos de CLI para lograr el mismo efecto.

```azurecli
# show all artifacts generated by a run
$ az ml history info -r <runid> -a <artifact/path>

# promote a particular artifact
$ az ml history promote -r <runid> -ap <artifact/prefix> -n <name of asset to create>
```
## <a name="next-steps"></a>Pasos siguientes
- Revise [Clasificación del tutorial de IRIS, parte 2](tutorial-classifying-iris-part-2.md) para ver la API de registro en acción.
- Revise [Cómo utilizar el historial de ejecución y las métricas de modelo en Azure Machine Learning Workbench](how-to-use-run-history-model-metrics.md) para comprender un poco más cómo se puede usar el registro de las API en el historial de ejecución.
