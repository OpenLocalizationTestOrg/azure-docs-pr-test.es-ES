---
title: "Tutorial de Data Factory: primera canalización de datos | Microsoft Docs"
description: "En este tutorial de Data Factory de Azure se muestra cómo crear y programar una factoría de datos que procese los datos mediante el script de Hive en un clúster de Hadoop."
services: data-factory
keywords: "Tutorial de Data Factory de Azure, clúster de Hadoop, Hive de Hadoop"
documentationcenter: 
author: spelluru
manager: jhubbard
editor: 
ms.assetid: 81f36c76-6e78-4d93-a3f2-0317b413f1d0
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 07/10/2017
ms.author: spelluru
ms.openlocfilehash: 08e2988d455cca21726162d9fb128e91fd51f463
ms.sourcegitcommit: 18ad9bc049589c8e44ed277f8f43dcaa483f3339
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 08/29/2017
---
# <a name="tutorial-build-your-first-pipeline-to-transform-data-using-hadoop-cluster"></a><span data-ttu-id="ab1b4-104">Tutorial: Compilación de la primera canalización para procesar datos mediante el clúster de Hadoop</span><span class="sxs-lookup"><span data-stu-id="ab1b4-104">Tutorial: Build your first pipeline to transform data using Hadoop cluster</span></span>
> [!div class="op_single_selector"]
> * [<span data-ttu-id="ab1b4-105">Introducción y requisitos previos</span><span class="sxs-lookup"><span data-stu-id="ab1b4-105">Overview and prerequisites</span></span>](data-factory-build-your-first-pipeline.md)
> * [<span data-ttu-id="ab1b4-106">Portal de Azure</span><span class="sxs-lookup"><span data-stu-id="ab1b4-106">Azure portal</span></span>](data-factory-build-your-first-pipeline-using-editor.md)
> * [<span data-ttu-id="ab1b4-107">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="ab1b4-107">Visual Studio</span></span>](data-factory-build-your-first-pipeline-using-vs.md)
> * [<span data-ttu-id="ab1b4-108">PowerShell</span><span class="sxs-lookup"><span data-stu-id="ab1b4-108">PowerShell</span></span>](data-factory-build-your-first-pipeline-using-powershell.md)
> * [<span data-ttu-id="ab1b4-109">Plantilla de Resource Manager</span><span class="sxs-lookup"><span data-stu-id="ab1b4-109">Resource Manager template</span></span>](data-factory-build-your-first-pipeline-using-arm.md)
> * [<span data-ttu-id="ab1b4-110">API de REST</span><span class="sxs-lookup"><span data-stu-id="ab1b4-110">REST API</span></span>](data-factory-build-your-first-pipeline-using-rest-api.md)

<span data-ttu-id="ab1b4-111">En este tutorial, se crea una instancia de Azure Data Factory con una canalización de datos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-111">In this tutorial, you build your first Azure data factory with a data pipeline.</span></span> <span data-ttu-id="ab1b4-112">La canalización transforma los datos de entrada mediante la ejecución de un script de Hive en un clúster de Azure HDInsight (Hadoop) para generar datos de salida.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-112">The pipeline transforms input data by running Hive script on an Azure HDInsight (Hadoop) cluster to produce output data.</span></span>  

<span data-ttu-id="ab1b4-113">Este artículo se proporciona información general del tutorial y se indican sus requisitos previos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-113">This article provides overview and prerequisites for the tutorial.</span></span> <span data-ttu-id="ab1b4-114">Cuando se cumplan los requisitos previos, puede realizar el tutorial mediante uno de los siguientes SDK o herramientas: Azure Portal, Visual Studio, PowerShell, plantilla de Resource Manager o API de REST.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-114">After you complete the prerequisites, you can do the tutorial using one of the following tools/SDKs: Azure portal, Visual Studio, PowerShell, Resource Manager template, REST API.</span></span> <span data-ttu-id="ab1b4-115">Seleccione una de las opciones de la lista desplegable del principio o los vínculos del final de este artículo para realizar el tutorial mediante una de estas opciones.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-115">Select one of the options in the drop-down list at the beginning (or) links at the end of this article to do the tutorial using one of these options.</span></span>    

## <a name="tutorial-overview"></a><span data-ttu-id="ab1b4-116">Información general del tutorial</span><span class="sxs-lookup"><span data-stu-id="ab1b4-116">Tutorial overview</span></span>
<span data-ttu-id="ab1b4-117">En este tutorial, realizará los siguientes pasos:</span><span class="sxs-lookup"><span data-stu-id="ab1b4-117">In this tutorial, you perform the following steps:</span></span>

1. <span data-ttu-id="ab1b4-118">Crear una **factoría de datos**.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-118">Create a **data factory**.</span></span> <span data-ttu-id="ab1b4-119">Una factoría de datos puede contener una o varias canalizaciones de datos que mueven y procesan datos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-119">A data factory can contain one or more data pipelines that move and transform data.</span></span> 

    <span data-ttu-id="ab1b4-120">En este tutorial, se crea una canalización de la factoría de datos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-120">In this tutorial, you create one pipeline in the data factory.</span></span> 
2. <span data-ttu-id="ab1b4-121">Crear una **canalización**.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-121">Create a **pipeline**.</span></span> <span data-ttu-id="ab1b4-122">Una canalización puede tener una o varias actividades (ejemplos: actividad de copia, actividad de Hive en HDInsight).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-122">A pipeline can have one or more activities (Examples: Copy Activity, HDInsight Hive Activity).</span></span> <span data-ttu-id="ab1b4-123">Este ejemplo usa la actividad de Hive de HDInsight que ejecuta un script de Hive en un clúster de Hadoop de HDInsight.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-123">This sample uses the HDInsight Hive activity that runs a Hive script on a HDInsight Hadoop cluster.</span></span> <span data-ttu-id="ab1b4-124">El script crea primero una tabla que hace referencia a los datos de blog sin procesar almacenados en Azure Blob Storage y, después, divide los datos sin procesar por año y mes.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-124">The script first creates a table that references the raw web log data stored in Azure blob storage and then partitions the raw data by year and month.</span></span>

    <span data-ttu-id="ab1b4-125">En este tutorial, la canalización usa Actividad de Hive para transformar datos, para lo que ejecuta una consulta de Hive en un clúster de Hadoop de Azure HDInsight.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-125">In this tutorial, the pipeline uses the Hive Activity to transform data by running a Hive query on an Azure HDInsight Hadoop cluster.</span></span> 
3. <span data-ttu-id="ab1b4-126">Cree **servicios vinculados**.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-126">Create **linked services**.</span></span> <span data-ttu-id="ab1b4-127">Un servicio vinculado se crea para vincular un almacén de datos o servicio de proceso a la factoría de datos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-127">You create a linked service to link a data store or a compute service to the data factory.</span></span> <span data-ttu-id="ab1b4-128">Un almacén de datos como Almacenamiento de Azure contiene los datos de entrada y salida de las actividades de la canalización.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-128">A data store such as Azure Storage holds input/output data of activities in the pipeline.</span></span> <span data-ttu-id="ab1b4-129">Un servicio de proceso como un clúster de Hadoop de HDInsight procesa y transforma los datos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-129">A compute service such as HDInsight Hadoop cluster processes/transforms data.</span></span>

    <span data-ttu-id="ab1b4-130">En este tutorial se crean dos servicios vinculados: **Azure Storage** y **Azure HDInsight**.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-130">In this tutorial, you create two linked services: **Azure Storage** and **Azure HDInsight**.</span></span> <span data-ttu-id="ab1b4-131">El servicio vinculado Azure Storage vincula una cuenta de Azure Storage que contiene los datos de entrada/salida con la factoría de datos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-131">The Azure Storage linked service links an Azure Storage Account that holds the input/output data to the data factory.</span></span> <span data-ttu-id="ab1b4-132">El servicio vinculado Azure HDInsight vincula un clúster de Azure HDInsight que se utiliza para transformar los datos con la factoría de datos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-132">Azure HDInsight linked service links an Azure HDInsight cluster that is used to transform data to the data factory.</span></span> 
3. <span data-ttu-id="ab1b4-133">Crear **conjuntos de datos**de entrada y salida.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-133">Create input and output **datasets**.</span></span> <span data-ttu-id="ab1b4-134">Un conjunto de datos de entrada representa la entrada para una actividad de la canalización y un conjunto de datos de salida representa la salida de la actividad.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-134">An input dataset represents the input for an activity in the pipeline and an output dataset represents the output for the activity.</span></span>

    <span data-ttu-id="ab1b4-135">En este tutorial, los conjuntos de datos de entrada y salida especifican las ubicaciones de los datos de entrada y salida en Azure Blob Storage.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-135">In this tutorial, the input and output datasets specify locations of input and output data in the Azure Blob Storage.</span></span> <span data-ttu-id="ab1b4-136">El servicio vinculado Azure Storage especifica qué cuenta de Azure Storage se usa.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-136">The Azure Storage linked service specifies what Azure Storage Account is used.</span></span> <span data-ttu-id="ab1b4-137">Un conjunto de datos de entrada especifica el lugar en que se encuentran los archivos de entrada, mientras que un conjunto de datos de salida especifica el lugar en que se colocan los archivos de salida.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-137">An input dataset specifies where the input files are located and an output dataset specifies where the output files are placed.</span></span> 


<span data-ttu-id="ab1b4-138">Para obtener información general detallada de Azure Data Factory, consulte el artículo [Introducción a Azure Data Factory](data-factory-introduction.md).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-138">See [Introduction to Azure Data Factory](data-factory-introduction.md) article for a detailed overview of Azure Data Factory.</span></span>
  
<span data-ttu-id="ab1b4-139">Este es la **vista de diagrama** de la factoría de datos de ejemplo creada en este tutorial.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-139">Here is the **diagram view** of the sample data factory you build in this tutorial.</span></span> <span data-ttu-id="ab1b4-140">**MyFirstPipeline** tiene una actividad del tipo Hive que consume el conjunto de datos **AzureBlobInput** como entrada y genera el conjunto de datos **AzureBlobOutput** como salida.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-140">**MyFirstPipeline** has one activity of type Hive that consumes **AzureBlobInput** dataset as an input and produces **AzureBlobOutput** dataset as an output.</span></span> 

![Vista de diagrama en el tutorial de Data Factory](media/data-factory-build-your-first-pipeline/data-factory-tutorial-diagram-view.png)


<span data-ttu-id="ab1b4-142">En este tutorial, la carpeta **inputdata** del contenedor de blobs de Azure **adfgetstarted** contiene un archivo llamado input.log.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-142">In this tutorial, **inputdata** folder of the **adfgetstarted** Azure blob container contains one file named input.log.</span></span> <span data-ttu-id="ab1b4-143">Este archivo de registro tiene entradas de tres meses: enero, febrero y marzo de 2016.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-143">This log file has entries from three months: January, February, and March of 2016.</span></span> <span data-ttu-id="ab1b4-144">Aquí están las filas de ejemplo para cada mes en el archivo de entrada.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-144">Here are the sample rows for each month in the input file.</span></span> 

```
2016-01-01,02:01:09,SAMPLEWEBSITE,GET,/blogposts/mvc4/step2.png,X-ARR-LOG-ID=2ec4b8ad-3cf0-4442-93ab-837317ece6a1,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,53175,871 
2016-02-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
2016-03-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
```

<span data-ttu-id="ab1b4-145">Cuando la canalización procesa el archivo mediante la actividad de Hive de HDInsight, esta actividad ejecuta un script de Hive en el clúster de HDInsight que divide los datos de entrada por año y mes.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-145">When the file is processed by the pipeline with HDInsight Hive Activity, the activity runs a Hive script on the HDInsight cluster that partitions input data by year and month.</span></span> <span data-ttu-id="ab1b4-146">El script crea tres carpetas de salida que contienen un archivo con entradas de cada mes.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-146">The script creates three output folders that contain a file with entries from each month.</span></span>  

```
adfgetstarted/partitioneddata/year=2016/month=1/000000_0
adfgetstarted/partitioneddata/year=2016/month=2/000000_0
adfgetstarted/partitioneddata/year=2016/month=3/000000_0
```

<span data-ttu-id="ab1b4-147">De las líneas de ejemplo mostradas anteriormente, la primera de ellas (con 2016-01-01) se escribirá en el archivo 000000_0 en la carpeta month=1.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-147">From the sample lines shown above, the first one (with 2016-01-01) is written to the 000000_0 file in the month=1 folder.</span></span> <span data-ttu-id="ab1b4-148">De igual manera, la segunda se escribirá en el archivo de la carpeta month=2 y la tercera en el archivo de la carpeta month=3.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-148">Similarly, the second one is written to the file in the month=2 folder and the third one is written to the file in the month=3 folder.</span></span>  

## <a name="prerequisites"></a><span data-ttu-id="ab1b4-149">Requisitos previos</span><span class="sxs-lookup"><span data-stu-id="ab1b4-149">Prerequisites</span></span>
<span data-ttu-id="ab1b4-150">Antes de comenzar este tutorial, debe cumplir los siguientes requisitos previos:</span><span class="sxs-lookup"><span data-stu-id="ab1b4-150">Before you begin this tutorial, you must have the following prerequisites:</span></span>

1. <span data-ttu-id="ab1b4-151">**Suscripción de Azure** : si no tiene ninguna, puede crear una cuenta de evaluación gratuita en un par de minutos.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-151">**Azure subscription** - If you don't have an Azure subscription, you can create a free trial account in just a couple of minutes.</span></span> <span data-ttu-id="ab1b4-152">Consulte el artículo [Evaluación gratuita](https://azure.microsoft.com/pricing/free-trial/) sobre cómo puede obtener una cuenta de evaluación gratuita.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-152">See the [Free Trial](https://azure.microsoft.com/pricing/free-trial/) article on how you can obtain a free trial account.</span></span>
2. <span data-ttu-id="ab1b4-153">**Azure Storage**: se usa una cuenta de almacenamiento estándar de uso general de Azure para almacenar los datos de este tutorial.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-153">**Azure Storage** – You use a general-purpose standard Azure storage account for storing the data in this tutorial.</span></span> <span data-ttu-id="ab1b4-154">Si no dispone de una cuenta de almacenamiento estándar de uso general de Azure, vea el artículo [Crear una cuenta de almacenamiento](../storage/common/storage-create-storage-account.md#create-a-storage-account).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-154">If you don't have a general-purpose standard Azure storage account, see the [Create a storage account](../storage/common/storage-create-storage-account.md#create-a-storage-account) article.</span></span> <span data-ttu-id="ab1b4-155">Después de haber creado la cuenta de almacenamiento, anote el **nombre de la cuenta** y la **clave de acceso**.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-155">After you have created the storage account, note down the **account name** and **access key**.</span></span> <span data-ttu-id="ab1b4-156">Consulte [Visualización y copia de las claves de acceso de almacenamiento](../storage/common/storage-create-storage-account.md#view-and-copy-storage-access-keys).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-156">See [View, copy and regenerate storage access keys](../storage/common/storage-create-storage-account.md#view-and-copy-storage-access-keys).</span></span>
3. <span data-ttu-id="ab1b4-157">Descargue y revise el archivo de consulta de Hive (**HQL**), que se encuentra en: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql](https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-157">Download and review the Hive query file (**HQL**) located at: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql](https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql).</span></span> <span data-ttu-id="ab1b4-158">Esta consulta transforma los datos de entrada para generar datos de salida.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-158">This query transforms input data to produce output data.</span></span> 
4. <span data-ttu-id="ab1b4-159">Descargue y revise el archivo de entrada de ejemplo (**input.log**), que se encuentra en: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log](https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-159">Download and review the sample input file (**input.log**) located at: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log](https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log)</span></span>
5. <span data-ttu-id="ab1b4-160">Crear un contenedor de blobs denominado **adfgetstarted** en Azure Blob Storage.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-160">Create a blob container named **adfgetstarted** in your Azure Blob Storage.</span></span> 
6. <span data-ttu-id="ab1b4-161">Cargue el archivo **partitionweblogs.hql** en la carpeta **script** del contenedor **adfgetstarted**.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-161">Upload **partitionweblogs.hql** file to the **script** folder in the **adfgetstarted** container.</span></span> <span data-ttu-id="ab1b4-162">Use herramientas como el [Explorador de Microsoft Azure Storage](http://storageexplorer.com/).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-162">Use tools such as [Microsoft Azure Storage Explorer](http://storageexplorer.com/).</span></span> 
7. <span data-ttu-id="ab1b4-163">Cargue el archivo **input.log** en la carpeta **inputdata** del contenedor **adfgetstarted**.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-163">Upload **input.log** file to the **inputdata** folder in the **adfgetstarted** container.</span></span> 

<span data-ttu-id="ab1b4-164">Una vez que se cumplan los requisitos previos, seleccione uno de los siguientes SDK o herramientas para realizar el tutorial:</span><span class="sxs-lookup"><span data-stu-id="ab1b4-164">After you complete the prerequisites, select one of the following tools/SDKs to do the tutorial:</span></span> 

- [<span data-ttu-id="ab1b4-165">Azure Portal</span><span class="sxs-lookup"><span data-stu-id="ab1b4-165">Azure portal</span></span>](data-factory-build-your-first-pipeline-using-editor.md)
- [<span data-ttu-id="ab1b4-166">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="ab1b4-166">Visual Studio</span></span>](data-factory-build-your-first-pipeline-using-vs.md)
- [<span data-ttu-id="ab1b4-167">PowerShell</span><span class="sxs-lookup"><span data-stu-id="ab1b4-167">PowerShell</span></span>](data-factory-build-your-first-pipeline-using-powershell.md)
- [<span data-ttu-id="ab1b4-168">Plantilla de Resource Manager</span><span class="sxs-lookup"><span data-stu-id="ab1b4-168">Resource Manager template</span></span>](data-factory-build-your-first-pipeline-using-arm.md)
- [<span data-ttu-id="ab1b4-169">API de REST</span><span class="sxs-lookup"><span data-stu-id="ab1b4-169">REST API</span></span>](data-factory-build-your-first-pipeline-using-rest-api.md)

<span data-ttu-id="ab1b4-170">Azure Portal y Visual Studio proporcionan una forma de generar factorías de datos mediante GUI.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-170">Azure portal and Visual Studio provide GUI way of building your data factories.</span></span> <span data-ttu-id="ab1b4-171">Mientras que las opciones de PowerShell, la plantilla de Resource Manager y la API de REST proporcionan una forma de generar factorías de datos mediante scripts/programación.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-171">Whereas, PowerShell, Resource Manager Template, and REST API options provides scripting/programming way of building your data factories.</span></span>

> [!NOTE]
> <span data-ttu-id="ab1b4-172">En este tutorial, la canalización de datos transforma los datos de entrada para generar datos de salida.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-172">The data pipeline in this tutorial transforms input data to produce output data.</span></span> <span data-ttu-id="ab1b4-173">No copia los datos de un almacén de datos de origen a un almacén de datos de destino.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-173">It does not copy data from a source data store to a destination data store.</span></span> <span data-ttu-id="ab1b4-174">Para ver un tutorial acerca de cómo copiar datos mediante Azure Data Factory, consulte [Copia de datos de Blob Storage en SQL Database mediante Data Factory](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-174">For a tutorial on how to copy data using Azure Data Factory, see [Tutorial: Copy data from Blob Storage to SQL Database](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).</span></span>
> 
> <span data-ttu-id="ab1b4-175">Puede encadenar dos actividades (ejecutar una después de otra) haciendo que el conjunto de datos de salida de una actividad sea el conjunto de datos de entrada de la otra actividad.</span><span class="sxs-lookup"><span data-stu-id="ab1b4-175">You can chain two activities (run one activity after another) by setting the output dataset of one activity as the input dataset of the other activity.</span></span> <span data-ttu-id="ab1b4-176">Para más información, consulte [Programación y ejecución en Data Factory](data-factory-scheduling-and-execution.md).</span><span class="sxs-lookup"><span data-stu-id="ab1b4-176">See [Scheduling and execution in Data Factory](data-factory-scheduling-and-execution.md) for detailed information.</span></span> 





  
