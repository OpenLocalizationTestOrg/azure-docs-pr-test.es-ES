---
title: "Kernels para Jupyter Notebook en clústeres Spark en Azure HDInsight | Microsoft Docs"
description: "Obtenga información sobre los kernels de PySpark, PySpark3 y Spark que puede usar con el cuaderno de Jupyter Notebook disponible con clústeres Spark en Azure HDInsight."
keywords: Jupyter Notebook en Spark, Spark en Jupyter
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="0d000-104">Kernels para Jupyter Notebook en clústeres Spark en Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="0d000-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="0d000-105">Los clústeres de HDInsight Spark proporcionan kernels que se pueden utilizar con el cuaderno de Jupyter Notebook en Spark para probar las aplicaciones.</span><span class="sxs-lookup"><span data-stu-id="0d000-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="0d000-106">Un kernel es un programa que ejecuta e interpreta el código.</span><span class="sxs-lookup"><span data-stu-id="0d000-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="0d000-107">Estos son los tres kernels:</span><span class="sxs-lookup"><span data-stu-id="0d000-107">The three kernels are:</span></span>

- <span data-ttu-id="0d000-108">**PySpark** (para aplicaciones escritas en Python2)</span><span class="sxs-lookup"><span data-stu-id="0d000-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="0d000-109">**PySpark3** (para aplicaciones escritas en Python3)</span><span class="sxs-lookup"><span data-stu-id="0d000-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="0d000-110">**Spark** (para aplicaciones escritas en Scala)</span><span class="sxs-lookup"><span data-stu-id="0d000-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="0d000-111">En este artículo, aprenderá a usar estos kernels y las ventajas de utilizarlos.</span><span class="sxs-lookup"><span data-stu-id="0d000-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="0d000-112">Requisitos previos</span><span class="sxs-lookup"><span data-stu-id="0d000-112">Prerequisites</span></span>

* <span data-ttu-id="0d000-113">Un clúster de Apache Spark en HDInsight.</span><span class="sxs-lookup"><span data-stu-id="0d000-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="0d000-114">Para obtener instrucciones, vea [Creación de clústeres Apache Spark en HDInsight de Azure](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="0d000-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="0d000-115">Creación de un cuaderno de Jupyter Notebook en clústeres Spark de HDInsight</span><span class="sxs-lookup"><span data-stu-id="0d000-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="0d000-116">En [Azure Portal](https://portal.azure.com/), abra el clúster.</span><span class="sxs-lookup"><span data-stu-id="0d000-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="0d000-117">Consulte [Enumeración y visualización de clústeres](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) para obtener instrucciones.</span><span class="sxs-lookup"><span data-stu-id="0d000-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="0d000-118">El clúster se abre en una nueva hoja del portal.</span><span class="sxs-lookup"><span data-stu-id="0d000-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="0d000-119">Desde la sección **Vínculos rápidos**, haga clic en **Paneles de clúster** para abrir la hoja **Paneles de clúster**.</span><span class="sxs-lookup"><span data-stu-id="0d000-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="0d000-120">Si no ve la sección **Vínculos rápidos**, haga clic en **Introducción** en el menú izquierdo de la hoja.</span><span class="sxs-lookup"><span data-stu-id="0d000-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="0d000-121">![Jupyter Notebook en Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter Notebook en Spark")</span><span class="sxs-lookup"><span data-stu-id="0d000-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="0d000-122">Haga clic en **Jupyter Notebook**.</span><span class="sxs-lookup"><span data-stu-id="0d000-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="0d000-123">Cuando se le pida, escriba las credenciales del clúster.</span><span class="sxs-lookup"><span data-stu-id="0d000-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="0d000-124">También puede comunicarse con el cuaderno de Jupyter Notebook del clúster Spark si abre la siguiente dirección URL en el explorador.</span><span class="sxs-lookup"><span data-stu-id="0d000-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="0d000-125">Reemplace **CLUSTERNAME** por el nombre del clúster:</span><span class="sxs-lookup"><span data-stu-id="0d000-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="0d000-126">Haga clic en **Nuevo** y, después, haga clic en **Pyspark** **PySpark3** o **Spark** para crear un cuaderno.</span><span class="sxs-lookup"><span data-stu-id="0d000-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="0d000-127">Utilice el kernel de Spark para las aplicaciones de Scala, kernel PySpark para aplicaciones de Python2 y kernel PySpark para3 aplicaciones de Python3.</span><span class="sxs-lookup"><span data-stu-id="0d000-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="0d000-128">![Kernels de Jupyter Notebook en Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels de Jupyter Notebook en Spark")</span><span class="sxs-lookup"><span data-stu-id="0d000-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="0d000-129">Se abre un cuaderno con el kernel que seleccionó.</span><span class="sxs-lookup"><span data-stu-id="0d000-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="0d000-130">Ventajas de utilizar los kernels</span><span class="sxs-lookup"><span data-stu-id="0d000-130">Benefits of using the kernels</span></span>

<span data-ttu-id="0d000-131">Estas son algunas ventajas de usar los kernels nuevo con el cuaderno de Jupyter Notebook en clústeres Spark de HDInsight.</span><span class="sxs-lookup"><span data-stu-id="0d000-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="0d000-132">**Contextos preestablecidos**.</span><span class="sxs-lookup"><span data-stu-id="0d000-132">**Preset contexts**.</span></span> <span data-ttu-id="0d000-133">Gracias a los kernels de **PySpark**, **PySpark3** o **Spark**, no necesita establecer de forma explícita los contextos de Spark o Hive para poder empezar a trabajar con las aplicaciones,</span><span class="sxs-lookup"><span data-stu-id="0d000-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="0d000-134">ya que están disponibles de forma predeterminada.</span><span class="sxs-lookup"><span data-stu-id="0d000-134">These are available by default.</span></span> <span data-ttu-id="0d000-135">Estos contextos son:</span><span class="sxs-lookup"><span data-stu-id="0d000-135">These contexts are:</span></span>
   
   * <span data-ttu-id="0d000-136">**sc** : para el contexto Spark</span><span class="sxs-lookup"><span data-stu-id="0d000-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="0d000-137">**sqlContext** : para el contexto Hive</span><span class="sxs-lookup"><span data-stu-id="0d000-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="0d000-138">Por tanto, no tiene que ejecutar instrucciones como la siguiente para definir los contextos:</span><span class="sxs-lookup"><span data-stu-id="0d000-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="0d000-139">sc = SparkContext('yarn-client')  sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="0d000-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="0d000-140">En su lugar, puede utilizar directamente los contextos preestablecidos en la aplicación.</span><span class="sxs-lookup"><span data-stu-id="0d000-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="0d000-141">**Instrucciones mágicas de celda**.</span><span class="sxs-lookup"><span data-stu-id="0d000-141">**Cell magics**.</span></span> <span data-ttu-id="0d000-142">El kernel de PySpark proporciona algunas "instrucciones mágicas" predefinidas, que son comandos especiales que se pueden llamar con `%%` (por ejemplo, `%%MAGIC` <args>).</span><span class="sxs-lookup"><span data-stu-id="0d000-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="0d000-143">El comando mágico debe ser la primera palabra de una celda de código y permitir varias líneas de contenido.</span><span class="sxs-lookup"><span data-stu-id="0d000-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="0d000-144">La palabra mágica debe ser la primera palabra en la celda.</span><span class="sxs-lookup"><span data-stu-id="0d000-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="0d000-145">Si se agrega algo antes del comando mágico, incluso comentarios, se producirá un error.</span><span class="sxs-lookup"><span data-stu-id="0d000-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="0d000-146">Para obtener más información sobre instrucciones mágicas, vaya [aquí](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span><span class="sxs-lookup"><span data-stu-id="0d000-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="0d000-147">La siguiente tabla muestra las diferentes instrucciones mágicas disponibles a través de los kernels.</span><span class="sxs-lookup"><span data-stu-id="0d000-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="0d000-148">Instrucción mágica</span><span class="sxs-lookup"><span data-stu-id="0d000-148">Magic</span></span> | <span data-ttu-id="0d000-149">Ejemplo</span><span class="sxs-lookup"><span data-stu-id="0d000-149">Example</span></span> | <span data-ttu-id="0d000-150">Description</span><span class="sxs-lookup"><span data-stu-id="0d000-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="0d000-151">help</span><span class="sxs-lookup"><span data-stu-id="0d000-151">help</span></span> |`%%help` |<span data-ttu-id="0d000-152">Genera una tabla de todas las instrucciones mágicas disponibles con el ejemplo y la descripción</span><span class="sxs-lookup"><span data-stu-id="0d000-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="0d000-153">info</span><span class="sxs-lookup"><span data-stu-id="0d000-153">info</span></span> |`%%info` |<span data-ttu-id="0d000-154">Produce información de sesión del punto de conexión actual de Livy.</span><span class="sxs-lookup"><span data-stu-id="0d000-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="0d000-155">CONFIGURAR</span><span class="sxs-lookup"><span data-stu-id="0d000-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="0d000-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="0d000-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="0d000-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="0d000-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="0d000-158">Configura los parámetros para crear una sesión.</span><span class="sxs-lookup"><span data-stu-id="0d000-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="0d000-159">El marcador de fuerza (-f) es obligatorio si ya se ha creado una sesión, lo que garantiza que la sesión se elimina y se vuelve a crear.</span><span class="sxs-lookup"><span data-stu-id="0d000-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="0d000-160">Consulte [Livy's POST /sessions Request Body (Cuerpo de la solicitud de sesiones o POST de Livy)](https://github.com/cloudera/livy#request-body) para obtener una lista de parámetros válidos.</span><span class="sxs-lookup"><span data-stu-id="0d000-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="0d000-161">Los parámetros deben pasarse como una cadena JSON y deben estar en la siguiente línea después de la instrucción mágica, como se muestra en la columna de ejemplo.</span><span class="sxs-lookup"><span data-stu-id="0d000-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="0d000-162">sql</span><span class="sxs-lookup"><span data-stu-id="0d000-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="0d000-163">Ejecuta una consulta de Hive en el sqlContext.</span><span class="sxs-lookup"><span data-stu-id="0d000-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="0d000-164">Si se pasa el parámetro `-o` , el resultado de la consulta se conserva en el contexto %%local de Python como trama de datos [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="0d000-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="0d000-165">local</span><span class="sxs-lookup"><span data-stu-id="0d000-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="0d000-166">Todo el código de las líneas siguientes se ejecuta localmente.</span><span class="sxs-lookup"><span data-stu-id="0d000-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="0d000-167">El código debe ser código Python2 válido, independientemente del kernel que se utilice.</span><span class="sxs-lookup"><span data-stu-id="0d000-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="0d000-168">Por tanto, aunque haya seleccionado los kernels **PySpark3** o **Spark** al crear el cuaderno, si usa el comando mágico `%%local` en una celda, esta solo debe tener código Python2 válido...</span><span class="sxs-lookup"><span data-stu-id="0d000-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="0d000-169">logs</span><span class="sxs-lookup"><span data-stu-id="0d000-169">logs</span></span> |`%%logs` |<span data-ttu-id="0d000-170">Genera los registros de la sesión actual de Livy.</span><span class="sxs-lookup"><span data-stu-id="0d000-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="0d000-171">delete</span><span class="sxs-lookup"><span data-stu-id="0d000-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="0d000-172">Elimina una sesión específica del punto de conexión actual de Livy.</span><span class="sxs-lookup"><span data-stu-id="0d000-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="0d000-173">Tenga en cuenta que no se puede eliminar la sesión iniciada en el propio kernel.</span><span class="sxs-lookup"><span data-stu-id="0d000-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="0d000-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="0d000-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="0d000-175">Elimina todas las sesiones del punto de conexión actual de Livy, incluida la sesión de este cuaderno.</span><span class="sxs-lookup"><span data-stu-id="0d000-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="0d000-176">La marca force -f es obligatoria.</span><span class="sxs-lookup"><span data-stu-id="0d000-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="0d000-177">Además de las instrucciones mágicas agregadas mediante el kernel PySpark, también puede utilizar las [instrucciones integradas de IPython](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), como `%%sh`.</span><span class="sxs-lookup"><span data-stu-id="0d000-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="0d000-178">Puede usar la instrucción mágica `%%sh` para ejecutar scripts y el bloque de código en el nodo principal del clúster.</span><span class="sxs-lookup"><span data-stu-id="0d000-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="0d000-179">**Visualización automática**.</span><span class="sxs-lookup"><span data-stu-id="0d000-179">**Auto visualization**.</span></span> <span data-ttu-id="0d000-180">El kernel **Pyspark** visualiza automáticamente el resultado de las consultas de Hive y SQL.</span><span class="sxs-lookup"><span data-stu-id="0d000-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="0d000-181">Puede elegir entre diferentes tipos de visualizaciones, como tabla, circular, línea, área o barra.</span><span class="sxs-lookup"><span data-stu-id="0d000-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="0d000-182">Parámetros compatibles con la instrucción mágica %%sql</span><span class="sxs-lookup"><span data-stu-id="0d000-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="0d000-183">El comando mágico `%%sql` es compatible con distintos parámetros que se pueden usar para controlar el tipo de resultado que se obtiene al ejecutar consultas.</span><span class="sxs-lookup"><span data-stu-id="0d000-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="0d000-184">En la tabla siguiente se muestra el resultado.</span><span class="sxs-lookup"><span data-stu-id="0d000-184">The following table lists the output.</span></span>

| <span data-ttu-id="0d000-185">Parámetro</span><span class="sxs-lookup"><span data-stu-id="0d000-185">Parameter</span></span> | <span data-ttu-id="0d000-186">Ejemplo</span><span class="sxs-lookup"><span data-stu-id="0d000-186">Example</span></span> | <span data-ttu-id="0d000-187">Description</span><span class="sxs-lookup"><span data-stu-id="0d000-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="0d000-188">-o</span><span class="sxs-lookup"><span data-stu-id="0d000-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="0d000-189">Use este parámetro para conservar el resultado de la consulta en el contexto %%local de Python como trama de datos [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="0d000-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="0d000-190">El nombre de la variable de la trama de datos es el nombre de variable que especifique.</span><span class="sxs-lookup"><span data-stu-id="0d000-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="0d000-191">-q</span><span class="sxs-lookup"><span data-stu-id="0d000-191">-q</span></span> |`-q` |<span data-ttu-id="0d000-192">Úselo para desactivar visualizaciones de la celda.</span><span class="sxs-lookup"><span data-stu-id="0d000-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="0d000-193">Si no quiere visualizar de forma automática el contenido de una celda y simplemente quiere capturarlo como una trama de datos, use `-q -o <VARIABLE>`.</span><span class="sxs-lookup"><span data-stu-id="0d000-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="0d000-194">Si quiere desactivar visualizaciones sin capturar los resultados (por ejemplo, para ejecutar una consulta de SQL, como una instrucción `CREATE TABLE`), use `-q` sin especificar un argumento `-o`.</span><span class="sxs-lookup"><span data-stu-id="0d000-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="0d000-195">-m</span><span class="sxs-lookup"><span data-stu-id="0d000-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="0d000-196">Donde **METHOD** puede ser **take** o **sample** (el valor predeterminado es **take**).</span><span class="sxs-lookup"><span data-stu-id="0d000-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="0d000-197">Si el método es **take**, el kernel toma elementos de la parte superior del conjunto de datos de resultados especificado por MAXROWS (se describe más adelante en esta tabla).</span><span class="sxs-lookup"><span data-stu-id="0d000-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="0d000-198">Si el método es **sample**, el kernel toma como muestra los elementos del conjunto de datos de forma aleatoria en función del parámetro `-r`, que se describe a continuación en esta tabla.</span><span class="sxs-lookup"><span data-stu-id="0d000-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="0d000-199">-r</span><span class="sxs-lookup"><span data-stu-id="0d000-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="0d000-200">Aquí **FRACTION** es un número de punto flotante entre 0,0 y 1,0.</span><span class="sxs-lookup"><span data-stu-id="0d000-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="0d000-201">Si el método sample de la consulta SQL es `sample`, el kernel muestrea de forma aleatoria la fracción especificada de los elementos del conjunto de resultados establecido.</span><span class="sxs-lookup"><span data-stu-id="0d000-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="0d000-202">Por ejemplo, si ejecuta una consulta SQL con los argumentos `-m sample -r 0.01`, el 1 % de las filas del resultados se muestrean de forma aleatoria.</span><span class="sxs-lookup"><span data-stu-id="0d000-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="0d000-203">**MAXROWS** es un valor entero.</span><span class="sxs-lookup"><span data-stu-id="0d000-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="0d000-204">El kernel limita el número de filas de salida a **MAXROWS**.</span><span class="sxs-lookup"><span data-stu-id="0d000-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="0d000-205">Si **MAXROWS** tiene un valor negativo (por ejemplo, **-1**), no se limita el número de filas del conjunto de resultados.</span><span class="sxs-lookup"><span data-stu-id="0d000-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="0d000-206">**Ejemplo:**</span><span class="sxs-lookup"><span data-stu-id="0d000-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="0d000-207">La instrucción anterior hace lo siguiente:</span><span class="sxs-lookup"><span data-stu-id="0d000-207">The statement above does the following:</span></span>

* <span data-ttu-id="0d000-208">Selecciona todos los registros de **hivesampletable**.</span><span class="sxs-lookup"><span data-stu-id="0d000-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="0d000-209">Dado que se usa -q, desactiva la visualización automática.</span><span class="sxs-lookup"><span data-stu-id="0d000-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="0d000-210">Dado que se usa `-m sample -r 0.1 -n 500` , toma como muestra de forma aleatoria el 10 % de las filas de hivesampletable y limita el tamaño del conjunto de resultados a 500 filas.</span><span class="sxs-lookup"><span data-stu-id="0d000-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="0d000-211">Por último, dado que se ha usado `-o query2` , también guarda el resultado en una trama de datos denominada **query2**.</span><span class="sxs-lookup"><span data-stu-id="0d000-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="0d000-212">Consideraciones al utilizar los kernels nuevos</span><span class="sxs-lookup"><span data-stu-id="0d000-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="0d000-213">Sea cual sea el kernel que se use, dejar los cuadernos en ejecución consumirá los recursos del clúster.</span><span class="sxs-lookup"><span data-stu-id="0d000-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="0d000-214">Con estos kernels, dado que los contextos están preestablecidos, salir simplemente de los cuadernos no elimina el contexto y, por tanto, los recursos de clúster siguen estando en uso.</span><span class="sxs-lookup"><span data-stu-id="0d000-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="0d000-215">Una práctica recomendada es utilizar la opción **Close and Halt** (Cerrar y detener) del menú **File** (Archivo) del cuaderno cuando haya terminado de usar el cuaderno, que termina el contexto y, después, sale del cuaderno.</span><span class="sxs-lookup"><span data-stu-id="0d000-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="0d000-216">Estos son algunos ejemplos:</span><span class="sxs-lookup"><span data-stu-id="0d000-216">Show me some examples</span></span>

<span data-ttu-id="0d000-217">Al abrir un cuaderno de Jupyter Notebook, se ven dos carpetas disponibles en el nivel raíz.</span><span class="sxs-lookup"><span data-stu-id="0d000-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="0d000-218">La carpeta **PySpark** tiene cuadernos de ejemplo en los que se utiliza el nuevo kernel de **Python**.</span><span class="sxs-lookup"><span data-stu-id="0d000-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="0d000-219">La carpeta **Scala** tiene cuadernos de ejemplo en los que se utiliza el nuevo kernel de **Spark**.</span><span class="sxs-lookup"><span data-stu-id="0d000-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="0d000-220">Puede abrir el cuaderno **00 - [READ ME FIRST] Spark Magic Kernel Features** de la carpeta **PySpark** o **Spark** para conocer las diferentes instrucciones mágicas que están disponibles.</span><span class="sxs-lookup"><span data-stu-id="0d000-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="0d000-221">Puede usar los demás cuadernos de ejemplo disponibles en las dos carpetas para aprender a lograr distintos escenarios con cuadernos de Jupyter con clústeres de HDInsight Spark.</span><span class="sxs-lookup"><span data-stu-id="0d000-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="0d000-222">Almacenamiento de los cuadernos</span><span class="sxs-lookup"><span data-stu-id="0d000-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="0d000-223">Los cuadernos de Jupyter Notebook se guardan en la cuenta de almacenamiento asociada al clúster en la carpeta **/HdiNotebooks** .</span><span class="sxs-lookup"><span data-stu-id="0d000-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="0d000-224">Es posible acceder a los cuadernos, los archivos de texto y las carpetas que se crean en Jupyter desde la cuenta de almacenamiento.</span><span class="sxs-lookup"><span data-stu-id="0d000-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="0d000-225">Por ejemplo, si usa Jupyter para crear una carpeta **myfolder** y un cuaderno **myfolder/mynotebook.ipynb**, puede acceder a él en `/HdiNotebooks/myfolder/mynotebook.ipynb` dentro de la cuenta de almacenamiento.</span><span class="sxs-lookup"><span data-stu-id="0d000-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="0d000-226">También ocurre lo contrario, es decir, si carga un cuaderno directamente en la cuenta de almacenamiento en `/HdiNotebooks/mynotebook1.ipynb`, se ve también desde Jupyter.</span><span class="sxs-lookup"><span data-stu-id="0d000-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="0d000-227">Los cuadernos permanecen en la cuenta de almacenamiento incluso después de que se elimine el clúster.</span><span class="sxs-lookup"><span data-stu-id="0d000-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="0d000-228">La forma de guardar los cuadernos en la cuenta de almacenamiento es compatible con HDFS.</span><span class="sxs-lookup"><span data-stu-id="0d000-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="0d000-229">Por lo tanto, si se usa SSH en el clúster, puede usar comandos de administración de archivos como se muestra en el siguiente fragmento de código:</span><span class="sxs-lookup"><span data-stu-id="0d000-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="0d000-230">En caso de que haya problemas para acceder a la cuenta de almacenamiento del clúster, los cuadernos también se guardan en el nodo principal `/var/lib/jupyter`.</span><span class="sxs-lookup"><span data-stu-id="0d000-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="0d000-231">Explorador compatible</span><span class="sxs-lookup"><span data-stu-id="0d000-231">Supported browser</span></span>

<span data-ttu-id="0d000-232">Los cuadernos de Jupyter Notebook que se ejecutan en clústeres Spark de HDInsight solo son compatibles con Google Chrome.</span><span class="sxs-lookup"><span data-stu-id="0d000-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="0d000-233">Comentarios</span><span class="sxs-lookup"><span data-stu-id="0d000-233">Feedback</span></span>
<span data-ttu-id="0d000-234">El nuevo kernel está en la fase de evolución y se desarrollará con el tiempo.</span><span class="sxs-lookup"><span data-stu-id="0d000-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="0d000-235">También podría significar que las API podrían cambiar a medida que estos kernels maduran.</span><span class="sxs-lookup"><span data-stu-id="0d000-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="0d000-236">Agradecemos cualquier comentario que tenga al utilizar estos nuevos kernels.</span><span class="sxs-lookup"><span data-stu-id="0d000-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="0d000-237">Esto resulta muy útil para dar forma a la versión final de estos kernels.</span><span class="sxs-lookup"><span data-stu-id="0d000-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="0d000-238">Puede dejar sus comentarios la sección **Comentarios** al final de este artículo.</span><span class="sxs-lookup"><span data-stu-id="0d000-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="0d000-239"><a name="seealso"></a>Otras referencias</span><span class="sxs-lookup"><span data-stu-id="0d000-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="0d000-240">Introducción a Apache Spark en HDInsight de Azure</span><span class="sxs-lookup"><span data-stu-id="0d000-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="0d000-241">Escenarios</span><span class="sxs-lookup"><span data-stu-id="0d000-241">Scenarios</span></span>
* [<span data-ttu-id="0d000-242">Spark with BI: Realizar el análisis de datos interactivos con Spark en HDInsight con las herramientas de BI</span><span class="sxs-lookup"><span data-stu-id="0d000-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="0d000-243">Creación de aplicaciones de Aprendizaje automático con Apache Spark en HDInsight de Azure</span><span class="sxs-lookup"><span data-stu-id="0d000-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="0d000-244">Spark con aprendizaje automático: uso de Spark en HDInsight para predecir los resultados de la inspección de alimentos</span><span class="sxs-lookup"><span data-stu-id="0d000-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="0d000-245">Streaming con Spark: uso de Spark en HDInsight para compilar aplicaciones de streaming en tiempo real</span><span class="sxs-lookup"><span data-stu-id="0d000-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="0d000-246">Análisis del registro del sitio web con Spark en HDInsight</span><span class="sxs-lookup"><span data-stu-id="0d000-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="0d000-247">Creación y ejecución de aplicaciones</span><span class="sxs-lookup"><span data-stu-id="0d000-247">Create and run applications</span></span>
* [<span data-ttu-id="0d000-248">Crear una aplicación independiente con Scala</span><span class="sxs-lookup"><span data-stu-id="0d000-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="0d000-249">Ejecutar trabajos de forma remota en un clúster de Spark mediante Livy</span><span class="sxs-lookup"><span data-stu-id="0d000-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="0d000-250">Herramientas y extensiones</span><span class="sxs-lookup"><span data-stu-id="0d000-250">Tools and extensions</span></span>
* [<span data-ttu-id="0d000-251">Uso del complemento de herramientas de HDInsight para IntelliJ IDEA para crear y enviar aplicaciones de Spark Scala</span><span class="sxs-lookup"><span data-stu-id="0d000-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="0d000-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely (Uso del complemento de herramientas de HDInsight para IntelliJ IDEA para depurar aplicaciones de Spark de forma remota)</span><span class="sxs-lookup"><span data-stu-id="0d000-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="0d000-253">Uso de cuadernos de Zeppelin con un clúster Spark en HDInsight</span><span class="sxs-lookup"><span data-stu-id="0d000-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="0d000-254">Uso de paquetes externos con cuadernos de Jupyter Notebook</span><span class="sxs-lookup"><span data-stu-id="0d000-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="0d000-255">Instalación de un cuaderno de Jupyter Notebook en el equipo y conexión al clúster de Apache Spark en HDInsight de Azure</span><span class="sxs-lookup"><span data-stu-id="0d000-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="0d000-256">Administración de recursos</span><span class="sxs-lookup"><span data-stu-id="0d000-256">Manage resources</span></span>
* [<span data-ttu-id="0d000-257">Administración de recursos para el clúster Apache Spark en HDInsight de Azure</span><span class="sxs-lookup"><span data-stu-id="0d000-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="0d000-258">Track and debug jobs running on an Apache Spark cluster in HDInsight (Seguimiento y depuración de trabajos que se ejecutan en un clúster de Apache Spark en HDInsight)</span><span class="sxs-lookup"><span data-stu-id="0d000-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
